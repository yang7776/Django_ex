# -*- coding: utf-8 -*-
# writer        Yang   
# create_time   2019/10/28 17:40
# file_name     Incremental_crawler.py

# todo 增量式爬虫
""""""
"""
增量式爬虫的核心就是：去重，但至于在哪个步骤去重，如何去重，都要根据情况而定
概念：通过爬虫程序监测某网站数据更新的情况，以便可以爬取到该网站更新出的新数据。
如何进行增量式的爬取工作：
	1、在发送请求之前判断这个URL是不是之前爬取过
	2、在解析内容后判断这部分内容是不是之前爬取过
	3、写入存储介质时判断内容是不是已经在介质中存在
分析：
      增量爬取的核心是去重， 至于去重的操作在哪个步骤起作用，只能说各有利弊。实际上，前两种思路需要根据实际情况取一个（也可能都用）。第一种思路适合不断有新页面出现的网站，比如说小说的新章节，每天的最新新闻等等；第二种思路则适合页面内容会更新的网站。第三个思路是相当于是最后的一道防线。这样做可以最大程度上达到去重的目的。

去重方法：
	注意：一般牵涉到“增量式爬虫”，数据一般都存在redis，而不是mongodb中，因为增量式爬虫对应爬取的数据，要满足可持久性存储。
	根据情况进行是否配置scrapy-redis，如果爬取临时数据，就不需要配置，直接在mongodb中判断即可
	1、利用scrapy-redis组件，在settings中配置内部自带的去重，原理：将爬取过程中产生的数据生成唯一key值进行存储，存储在redis的set中。当下次进行数据爬取时，首先对即将要发起的请求对应的url在存储的url的set中做判断，如果存在则不进行请求，否则才进行请求。   博客链接：https://blog.csdn.net/Hepburn_li/article/details/81480616
	2、自定义去重，分析网页的特性，将该唯一表示存储至redis的set中，当下次爬取到网页数据的时候，在进行持久化存储之前，首先可以先判断该数据的唯一标识在redis的set中是否存在，在决定是否进行持久化存储。

"""